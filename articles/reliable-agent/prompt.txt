is agent reliable?
can you list some typical mistakes?
what are the pains of developing agent system?

Reorganize the content as a standalone blog post

* do not reference the original articles, do not reference the original podcast, the reader do not have access to the original articles or podcast. 
* The blog post should NEVER use these words "首先" "其次" "再次" "最后". 
* The blog post should be very long
* Provide examples and analogies if necessary, but still keep technical details
* Reader should learn enough HOW-TO from your blog post
* The blog post is written in Chinese

===

构建产品 Copilot 的挑战与机遇

随着人工智能技术的飞速发展,越来越多的公司开始将先进的 AI 能力嵌入到他们的产品中。这些被称为"Copilot"的智能助手,让用户可以用自然语言提问,并获得与用户上下文相关的回答。事实上,几乎每一家大型科技公司都在努力为他们的软件产品增加这些功能。

然而,对于大多数软件工程师来说,这通常是他们第一次接触并集成 AI 驱动的技术。此外,现有的软件工程流程和工具还没有跟上构建 AI 应用所面临的挑战和规模。在本文中,我们采访了 26 位来自不同公司、负责构建产品 Copilot 的专业软件工程师,分享了他们在工程过程中遇到的痛点,以及给整个软件工程社区带来的机遇和工具设计思路。

提示工程是一个充满不确定性的过程

对于大多数参与者来说,编写提示语(prompt)和评估输出结果的过程,就像在 OpenAI 等平台提供的 playground 中反复试错。他们会随意尝试不同的提示,观察输出,然后不断调整,但很难获得一致的结果。这个过程很快就会"变成一场噩梦",因为工程师需要考虑各种边界情况,以及在提示中平滑地融入各种物理和上下文属性。

此外,即便是一个看似合理的提示,要想获得机器可读的输出,或者系统地解析文本以在产品中使用,往往还需要再次进行大量迭代,甚至推翻之前的方法。比如有的工程师尝试在提示中给出 JSON schema 作为期望的输出格式,但有时模型会生成不符合 schema 的对象,或者臆想出没有告知它的停止标记。

随着经验的积累,工程师们发现了一些更有效的策略,比如使用 markdown 等人类可读的格式,因为模型更倾向于输出带有标题、要点等格式的内容。如果强行让模型生成它不习惯的格式,反而会导致更高的错误率。

此外,在提示中平衡上下文信息和 token 数量限制,也是一个不小的挑战。用户在与 Copilot 交互时,通常会给出一些短语,比如"重构这段代码"或"给表格加上边框"。要让 Copilot 正确理解用户任务和环境的上下文,需要工程师精心设计提示。然而,提供合适的上下文并非易事,工程师需要从海量数据中提取关键信息,并压缩到尽可能少的 token 中。同时,测试提示的不同部分对整体任务性能的影响,也很困难。

最后,随着提示的日益复杂,工程师开始将其拆分为示例、指令/规则、模板等组件,形成一个提示库。虽然这带来了动态组装提示的灵活性,但也引入了新的挑战:很难检查最终生成的完整提示;缺乏持续验证和跟踪提示性能的机制,特别是评估对提示或模型的微调的影响。

编排多个提示和评估结果是另一个痛点

对于许多产品 Copilot 来说,要在产品中执行操作或收集与任务相关的信息,通常需要编排和评估多个提示。首先是进行意图检测,判断用户想要做什么,然后将提示路由到相应的技能组件。获得初步结果后,还需要进一步处理,比如判断是要更新当前选中的代码,还是在下方插入新代码。

不幸的是,Copilot 能执行的命令通常非常有限。从 Copilot 告诉用户如何配置环境,到真正帮用户完成配置,似乎是合乎逻辑的下一步。但让 Copilot 不经用户干预就自动执行操作,是有风险的,因为它生成的内容并不可靠,用户在下一步行动前还需要仔细审查。

对于基于意图或技能路由架构的 Copilot,进行长对话或处理简单的跟进问题,往往是不可能的。因为提示和上下文是由路由的技能指令自动填充的,这破坏了自然对话的流畅性。一些参与者尝试了基于 agent 的方法,将大语言模型视为一个 agent,在观察和思考后采取行动。但他们也指出,虽然这种方法"更强大",但"其行为真的很难管理和控制"。此外,这些 agent 很容易陷入循环或偏离正轨。它们很难准确判断何时完成了一项任务。工程师们呼吁,需要更好地洞察 agent 的内部推理状态,跟踪多步骤任务,并为 agent 行为设置更强的护栏。

测试和评估是一个巨大的挑战

面对生成式模型,传统的单元测试方法很快遇到了困难。因为每次运行测试,模型生成的结果可能都不一样,就像每个测试用例都是不稳定的。工程师不得不多次运行每个测试,比如运行10次,只有7次通过才算成功。他们还必须以实验的心态来评估输入,因为一个场景下有效的输入,并不保证在另一个场景下也有效。

此外,在进行回归测试或评估不同模型和 agent 设计之间的性能差异时,大家都希望使用基准测试来做决策。但面临两个问题:1)根本没有现成的基准,每个人都得自己想办法创建;2)缺乏明确的指标来判断什么是"足够好"或"更好"的性能。

有人建议,对定性的输出进行人工评估,请评估者给出简单的是或否。但创建人工标注的数据集成本很高,需要外包,因为内部完成的话,"单调乏味且耗时"。即便建立了基准测试,要将其集成到软件工程流程中也面临资源限制的挑战。运行大量测试输入的成本很高,有的工程师被要求停止自动化测试的努力,改为在有大的变更后手动运行一小部分测试。

确定多少性能才算"足够好",是另一个令人头疼的问题。有人感慨:"我们如何在不过度投入资源追求完美的情况下,确定我们达到了正确的结果?"也有人提出了一个简单的评分方案,给出 A、B 等级,尽管主观有偏差,但对多人评分取平均可以缓解。

构建 Copilot 需要全新的知识和最佳实践

对于许多参与者来说,他们必须从头学起,摸着石头过河。他们借助社交媒体上新兴的实践社区,关注相关话题标签,阅读他人分享的提示示例,比较不同项目的做法和使用的工具,来自学。有人甚至使用 GPT-4 本身作为学习的辅助工具,让它审查自己写的代码并提供改进建议,大大减少了学习曲线。

然而,这些知识往往是短暂和不稳定的。生态系统和底层模型变化太快,投资学习资源的回报存疑。有人质疑正在学习的新技能能持续多久,"提示工程是一项全新的技能,我们不知道它还能存在多久"。大家还强调,缺乏关于最佳实践的权威信息,总有一种"现在下定论还为时尚早"的感觉,以及担心某些工作岗位可能会被淘汰。

对一些人来说,他们意识到必须从根本上改变解决问题和构建系统的方式。正如一位参与者所言:"对于新来的人,他们必须以开放的心态来对待,他们需要抛开以前学到的一切,重新思考。你不能期望得到确定性的响应,这对很多人来说是可怕的。不存在100%正确的答案。你可能只改变提示中的一个词,整个体验就可能是错的。测试的概念不再是你以为的那样。不可能有100%通过测试的情况了。"

尽管如此,大家还是非常渴望尽快建立起最佳实践,让他们可以回到"专注于想法本身,快速让客户见到成果"的正轨上来。

隐私、安全和合规是不容忽视的问题

确保用户安全和设置"护栏"是工程师的重中之重。有人描述"将权力交给 AI 是多么可怕",因为 Windows 系统运行在核电站里。一个常见的策略是检测离题的请求,但聊天很容易偏离正轨。比如 Copilot 会误将用户的反馈当作对上一步

Citations:
[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/14818591/04123aa3-096d-47be-ac00-f5f4837d61a7/paste.txt


操作的指令。为了缓解这些问题,一些公司要求产品 Copilot 在所有请求中调用带有内容过滤的托管端点。然而,这些并不总是足够的,导致一些工程师不得不使用基于规则的分类器和人工防护列表,以防止"向客户显示某些词汇或短语"(P10)。

另一个复杂性的来源是确保输入到模型中以及从模型中检索到的输出都尊重隐私和安全。例如,P7 不得不添加额外的处理以确保"模型的输出不能包含在我们整个系统环境中容易检索到的标识符"。有时,在平衡来自第三方模型主机的策略时,这会变得更加复杂。一位参与者透露:"事实上,我们与 OpenAI 有合作关系,他们会为我们托管一个内部模型,因为他们的政策是可以将任何对话作为训练数据摄取的,这对我们来说是一个巨大的合规风险。"

不幸的是,由于遥测数据的悖论情况,确保安全和隐私变得更加困难,遥测数据通常用于记录事件和功能使用情况(Barik et al.,2016)。对于大多数软件工程师来说,例如 P14,"遥测是了解用户如何与 Copilot 交互的理想方式"。但正如 P2 解释的那样,"我们有遥测,但我们看不到用户提示,只能看到后端运行的内容,比如使用了哪些技能。例如,我们知道解释技能是最常用的,但不知道用户要求解释什么。" P4 总结说,"遥测数据将不够用;我们需要更好地了解正在生成的内容。"

虽然一些软件工程师有进行隐私和安全审查的经验,但对大多数软件工程师来说,执行负责任的 AI 评估——合规性和安全性审查——是一种新的体验。

P3 描述了他们的经历,首先从一个"影响评估"开始。该评估需要阅读几十页内容以了解"安全标准并知道您的系统是否符合这些标准。我花了1-2天的时间专注于此。"然后,他们与 AI 评估教练会面:"第一次会议持续了3.5小时,进行了大量讨论"。结果是"一堆工作项,大量必需的文档,还有更多工作要做。"与其他安全或隐私审查相比,通常需要1-2天,而对于 P3 来说,整个过程需要两周的工作。P24 也经历了负责任的 AI 评估,其中一个主要结果是需要生成一个自动化基准,以确保端点的内容过滤器标记了涉及几类危害的任何内容,包括仇恨、自残和暴力,每一类都涉及数百个子类别。对于 P24 来说,这是最高优先级——"在完成之前我们无法发布"。

总之,确保用户安全和设置适当的"护栏"是工程师在构建产品 Copilot 时面临的最大挑战之一。常见的问题包括聊天容易偏离正轨,需要使用基于规则的分类器和人工防护列表来过滤不当内容。此外,输入和输出数据的隐私和安全也需要特别注意,但遥测数据的使用又受到限制,给问题排查带来困难。负责任的 AI 评估是一个全新的领域,需要大量的时间投入,并可能阻碍产品的发布进度。这些都凸显了在 Copilot 开发过程中,工程师需要在功能创新和安全合规之间寻求平衡。

Citations:
[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/14818591/04123aa3-096d-47be-ac00-f5f4837d61a7/paste.txt

===

# 构建可靠的产品 Copilot:挑战与机遇

随着人工智能技术的飞速发展,越来越多的公司开始将先进的 AI 能力嵌入到他们的产品中。这些被称为"Copilot"的智能助手,让用户可以用自然语言提问,并获得与用户上下文相关的回答。事实上,几乎每一家大型科技公司都在努力为他们的软件产品增加这些功能。

然而,对于大多数软件工程师来说,这通常是他们第一次接触并集成 AI 驱动的技术。此外,现有的软件工程流程和工具还没有跟上构建 AI 应用所面临的挑战和规模。本文将分享一些在工程过程中遇到的痛点,以及给整个软件工程社区带来的机遇和工具设计思路。

## 提示工程是一个充满不确定性的过程

对于大多数参与者来说,编写提示语(prompt)和评估输出结果的过程,就像在 OpenAI 等平台提供的 playground 中反复试错。他们会随意尝试不同的提示,观察输出,然后不断调整,但很难获得一致的结果。这个过程很快就会"变成一场噩梦",因为工程师需要考虑各种边界情况,以及在提示中平滑地融入各种物理和上下文属性。

此外,即便是一个看似合理的提示,要想获得机器可读的输出,或者系统地解析文本以在产品中使用,往往还需要再次进行大量迭代,甚至推翻之前的方法。比如有的工程师尝试在提示中给出 JSON schema 作为期望的输出格式,但有时模型会生成不符合 schema 的对象,或者臆想出没有告知它的停止标记。

随着经验的积累,工程师们发现了一些更有效的策略,比如使用 markdown 等人类可读的格式,因为模型更倾向于输出带有标题、要点等格式的内容。如果强行让模型生成它不习惯的格式,反而会导致更高的错误率。

此外,在提示中平衡上下文信息和 token 数量限制,也是一个不小的挑战。用户在与 Copilot 交互时,通常会给出一些短语,比如"重构这段代码"或"给表格加上边框"。要让 Copilot 正确理解用户任务和环境的上下文,需要工程师精心设计提示。然而,提供合适的上下文并非易事,工程师需要从海量数据中提取关键信息,并压缩到尽可能少的 token 中。同时,测试提示的不同部分对整体任务性能的影响,也很困难。

最后,随着提示的日益复杂,工程师开始将其拆分为示例、指令/规则、模板等组件,形成一个提示库。虽然这带来了动态组装提示的灵活性,但也引入了新的挑战:很难检查最终生成的完整提示;缺乏持续验证和跟踪提示性能的机制,特别是评估对提示或模型的微调的影响。

## 编排多个提示和评估结果是另一个痛点  

对于许多产品 Copilot 来说,要在产品中执行操作或收集与任务相关的信息,通常需要编排和评估多个提示。这包括进行意图检测,判断用户想要做什么,然后将提示路由到相应的技能组件。获得初步结果后,还需要进一步处理,比如判断是要更新当前选中的代码,还是在下方插入新代码。

不幸的是,Copilot 能执行的命令通常非常有限。从 Copilot 告诉用户如何配置环境,到真正帮用户完成配置,似乎是合乎逻辑的下一步。但让 Copilot 不经用户干预就自动执行操作,是有风险的,因为它生成的内容并不可靠,用户在下一步行动前还需要仔细审查。

对于基于意图或技能路由架构的 Copilot,进行长对话或处理简单的跟进问题,往往是不可能的。因为提示和上下文是由路由的技能指令自动填充的,这破坏了自然对话的流畅性。一些参与者尝试了基于 agent 的方法,将大语言模型视为一个 agent,在观察和思考后采取行动。但他们也指出,虽然这种方法"更强大",但"其行为真的很难管理和控制"。此外,这些 agent 很容易陷入循环或偏离正轨。它们很难准确判断何时完成了一项任务。工程师们呼吁,需要更好地洞察 agent 的内部推理状态,跟踪多步骤任务,并为 agent 行为设置更强的护栏。

## 测试和评估是一个巨大的挑战

面对生成式模型,传统的单元测试方法很快遇到了困难。因为每次运行测试,模型生成的结果可能都不一样,就像每个测试用例都是不稳定的。工程师不得不多次运行每个测试,比如运行10次,只有7次通过才算成功。他们还必须以实验的心态来评估输入,因为一个场景下有效的输入,并不保证在另一个场景下也有效。

此外,在进行回归测试或评估不同模型和 agent 设计之间的性能差异时,大家都希望使用基准测试来做决策。但面临两个问题:1)根本没有现成的基准,每个人都得自己想办法创建;2)缺乏明确的指标来判断什么是"足够好"或"更好"的性能。

有人建议,对定性的输出进行人工评估,请评估者给出简单的是或否。但创建人工标注的数据集成本很高,需要外包,因为内部完成的话,"单调乏味且耗时"。即便建立了基准测试,要将其集成到软件工程流程中也面临资源限制的挑战。运行大量测试输入的成本很高,有的工程师被要求停止自动化测试的努力,改为在有大的变更后手动运行一小部分测试。

确定多少性能才算"足够好",是另一个令人头疼的问题。有人感慨:"我们如何在不过度投入资源追求完美的情况下,确定我们达到了正确的结果?"也有人提出了一个简单的评分方案,给出 A、B 等级,尽管主观有偏差,但对多人评分取平均可以缓解。

## 构建 Copilot 需要全新的知识和最佳实践

对于许多参与者来说,他们必须从头学起,摸着石头过河。他们借助社交媒体上新兴的实践社区,关注相关话题标签,阅读他人分享的提示示例,比较不同项目的做法和使用的工具,来自学。有人甚至使用 GPT-4 本身作为学习的辅助工具,让它审查自己写的代码并提供改进建议,大大减少了学习曲线。

然而,这些知识往往是短暂和不稳定的。生态系统和底层模型变化太快,投资学习资源的回报存疑。有人质疑正在学习的新技能能持续多久,"提示工程是一项全新的技能,我们不知道它还能存在多久"。大家还强调,缺乏关于最佳实践的权威信息,总有一种"现在下定论还为时尚早"的感觉,以及担心某些工作岗位可能会被淘汰。

对一些人来说,他们意识到必须从根本上改变解决问题和构建系统的方式。正如一位参与者所言:"对于新来的人,他们必须以开放的心态来对待,他们需要抛开以前学到的一切,重新思考。你不能期望得到确定性的响应,这对很多人来说是可怕的。不存在100%正确的答案。你可能只改变提示中的一个词,整个体验就可能是错的。测试的概念不再是你以为的那样。不可能有100%通过测试的情况了。"

## 隐私、安全和合规是不容忽视的问题

确保用户安全和设置"护栏"是工程师的重中之重。有人描述"将权力交给 AI 是多么可怕",因为 Windows 系统运行在核电站里。一个常见的策略是检测离题的请求,但聊天很容易偏离正轨。比如 Copilot 会误将用户的反馈当作对上一步操作的指令。为了缓解这些问题,一些公司要求产品 Copilot 在所有请求中调用带有内容过滤的托管端点。然而,这些并不总是足够的,导致一些工程师不得不使用基于规则的分类器和人工防护列表,以防止"向客户显示某些词汇或短语"。

另一个复杂性的

Citations:
[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/14818591/67ca241a-99bd-46af-a258-da061191b76f/paste.txt

来源是确保输入到模型中以及从模型中检索到的输出都尊重隐私和安全。例如,有的参与者不得不添加额外的处理以确保"模型的输出不能包含在我们整个系统环境中容易检索到的标识符"。有时,在平衡来自第三方模型主机的策略时,这会变得更加复杂。一位参与者透露:"事实上,我们与 OpenAI 有合作关系,他们会为我们托管一个内部模型,因为他们的政策是可以将任何对话作为训练数据摄取的,这对我们来说是一个巨大的合规风险。"

不幸的是,由于遥测数据的悖论情况,确保安全和隐私变得更加困难,遥测数据通常用于记录事件和功能使用情况。对于大多数软件工程师来说,遥测是了解用户如何与 Copilot 交互的理想方式。但正如有人解释的那样,"我们有遥测,但我们看不到用户提示,只能看到后端运行的内容,比如使用了哪些技能。例如,我们知道解释技能是最常用的,但不知道用户要求解释什么。" 有人总结说,"遥测数据将不够用;我们需要更好地了解正在生成的内容。"

虽然一些软件工程师有进行隐私和安全审查的经验,但对大多数软件工程师来说,执行负责任的 AI 评估——合规性和安全性审查——是一种新的体验。

有人描述了他们的经历,首先从一个"影响评估"开始。该评估需要阅读几十页内容以了解"安全标准并知道您的系统是否符合这些标准。我花了1-2天的时间专注于此。"然后,他们与 AI 评估教练会面:"第一次会议持续了3.5小时,进行了大量讨论"。结果是"一堆工作项,大量必需的文档,还有更多工作要做。"与其他安全或隐私审查相比,通常需要1-2天,而对于这个人来说,整个过程需要两周的工作。另一个人也经历了负责任的 AI 评估,其中一个主要结果是需要生成一个自动化基准,以确保端点的内容过滤器标记了涉及几类危害的任何内容,包括仇恨、自残和暴力,每一类都涉及数百个子类别。对他来说,这是最高优先级——"在完成之前我们无法发布"。

总之,确保用户安全和设置适当的"护栏"是工程师在构建产品 Copilot 时面临的最大挑战之一。常见的问题包括聊天容易偏离正轨,需要使用基于规则的分类器和人工防护列表来过滤不当内容。此外,输入和输出数据的隐私和安全也需要特别注意,但遥测数据的使用又受到限制,给问题排查带来困难。负责任的 AI 评估是一个全新的领域,需要大量的时间投入,并可能阻碍产品的发布进度。这些都凸显了在 Copilot 开发过程中,工程师需要在功能创新和安全合规之间寻求平衡。

Citations:
[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/14818591/67ca241a-99bd-46af-a258-da061191b76f/paste.txt